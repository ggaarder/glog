# Why sigmoid?

Why do we need to put the sigmoid function in every neurons in a neural network? Why not just weighted sum plus bias? Why do we need to normalize the value into [0,1]?

I then read a stackexchange ask which leads me to [a list of activation functions](flst), where I get names of two friends I frequently met:
- Identity
- Step

[flst]: stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons

Then I found that my question should be: Why Activation Functions?

[Wikipedia - Activation Function][en.wikipedia.org/wiki/Activation_function] answered my question. Firstly, the value will overflow after layers since not normalized. Secondly, "only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes", since that if the activation is linear, the whole network represents a linear function, which is very limited in solving problems.
