# My first Neural Network works!

At first it just doesn't learn, so I added a commandline argument to set the learning factor eta, and it really works! Here is the history:

```
         eta last time  loss
    1                -  539998.625000
    2               50  539998.375000
    3             5000  405220.281250
    4           500000  144573.718750
    5           400000  109157.992188
    6           400000  109602.296875  <-- ???
    7           700000  109157.992188  <-- same as 5???
    7           700000   60000.000000  <-- ???
    8          7000000   60000.000000  <-- ??????
At the beginning I set the weights and biases randomly in [0,1], but when I dump the network I found that some went to [-1, 1], but some went to become a three digit one, some four, some five, even some six! I didn't read the whole dump, so maybe there are bigger ones.
    9          7000000   60000.000000  <-- I thought it is because the eta is too large that all thetas overflowed, so the network learnt nothing. So I take a smaller value in the train.
   10           700000   60000.000000
   11           700000   60000.000000
   12            70000   60000.000000  Still too large?
   13             7000   60000.000000  ... Not small enough?
   14              700   60000.000000  ... ??????????????
   15               50   60000.000000  ........
   16                1   60000.000000  something went wrong
   17               .5   60000.000000 
$ md5sum ntwkarg
2ac6ea77539f111bfe4dc543242a4ba9  ntwkarg
   18               .1   60000.000000
$ md5sum ntwkarg
2ac6ea77539f111bfe4dc543242a4ba9  ntwkarg

Why?
   19              .02   60000.000000
$ md5sum ntwkarg
0c353c76783d9fb8ab7c917e21aef6b9  ntwkarg

Changed a bit after I set this big 5000000 eta, huh!
   20          5000000  60000.000000
$ md5sum ntwkarg
441f5972b2ca9c4e3ede37c6be4462ae  ntwkarg

want more?
   20        500000000  60000.000000
   21      50000000000                  overflowed. It says "49999998976.000000"
```

Wait.... Isn't 60000 the number of the train samples? So it means I had finished training! Oh! I finally got why they take a 1/n in the loss function!

After taking the 1/n it becomes 1. But why not the final loss zero, but one?

Let's create a new network storage and train it again.

```
$ ./a.out n
Creating new network storage in "ntwkarg"
$ ./a.out e1000000
training.....................................................................
Loss: 8.999970
$ ./a.out e1000000
training.....................................................................
Loss: 1.000000
$ ./a.out e1000000
training.....................................................................
Loss: 1.000000
```

Again! Why?

```
result: 0 correct: 5
^Mtrainingresult: 0 correct: 0
^Mtrainingresult: 0 correct: 4
^Mtrainingresult: 0 correct: 1
^Mtrainingresult: 0 correct: 9
^Mtrainingresult: 0 correct: 2
^Mtrainingresult: 0 correct: 1
^Mtrainingresult: 0 correct: 3
^Mtrainingresult: 0 correct: 1
^Mtrainingresult: 0 correct: 4
^Mtrainingresult: 0 correct: 3
^Mtrainingresult: 0 correct: 5
^Mtrainingresult: 0 correct: 3
^Mtrainingresult: 0 correct: 6
^Mtrainingresult: 0 correct: 1
^Mtrainingresult: 0 correct: 7
^Mtrainingresult: 0 correct: 2
...
```

... seems the network had learnt to *cheat*! It's funny. Let's see the detail.

```
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
^Mtraining0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
```

Woo! I thought the network learnt to set the first output neuron to one and all others to zero. I was wrong. Is this because we trained too fast? Try training slowlier.

Try eta=500000. Failed.

Try eta=500. Good news: after the first train it goes to 1.00 1.00 0.33 1.00 1.00 1.00 1.00 1.00 1.00 1.00. After the second 1.00 1.00 0.00 1.00 0.70 1.00 1.00 1.00 1.00 1.00. It is strange that whatever all input images give out the same output. So there should be something wrong in the input neuron. I'll check it tomorrow.

